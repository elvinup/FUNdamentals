{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing\n",
    "\n",
    "If we want to write a program that can take advantage of multiple CPUs, this is the way to go.\n",
    "\n",
    "\n",
    "# ES/Opensearch Benchmark Tool\n",
    "\n",
    "This tool can send bulk dummy data to a specified elasticsearch cluster to test how much and how fast data can be sent at once. It uses multiple cores (1 per process) to maximize how many requests can be sent to an opensearch cluster, essentially stress testing it.\n",
    "\n",
    "The ThreadPoolExecutor does multi-threading, but won't take advantage of multiple cores due to GIL in python, so it'll still be sequential work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, RequestsHttpConnection, helpers\n",
    "import os\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures as cf\n",
    "import boto3\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "\n",
    "def aws_client():\n",
    "    # Get Temporary Credentials\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    service = 'es'\n",
    "    region = 'us-west-2'\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "    return awsauth\n",
    "\n",
    "\n",
    "def es_client(awsauth):\n",
    "    # Connect to the Elasticsearch cluster\n",
    "    host = os.environ['DOMAIN']\n",
    "\n",
    "    es = Elasticsearch(\n",
    "        hosts=[{'host': host[8:-1], 'port': 443}],\n",
    "        http_auth=awsauth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection\n",
    "    )\n",
    "    return es\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Read arguments')\n",
    "    parser.add_argument('-t', '--total', type=int, help='total number of docs to create', required=True)\n",
    "    parser.add_argument('-s', '--chunk_size', type=int, help='Use Bulk API to index dummy data into specfied chunks', required=True)\n",
    "    parser.add_argument('-n', '--num_machines', type=int, help='Number of machines', required=True)\n",
    "    parser.add_argument('-i', '--machine_id', type=int, help='Id of machine (should be > 0 and <= num_machines', required=True)\n",
    "    parser.add_argument('-index', '--index_name', help='Name of index to add docs to', required=True)\n",
    "    parser.add_argument('-w', '--max_workers', type=int, help='Max number of workers or processes to spawn', required=True)\n",
    "    parser.add_argument('-c', '--create_index', help='Including this means this instance of the script will create the index (only the first machine should)', action='store_true')\n",
    "    args = vars(parser.parse_args())\n",
    "    return args\n",
    "\n",
    "\n",
    "def create_index(index_name, es):\n",
    "    # Set the settings for the index\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 3,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"field1\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"field2\": {\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"field3\": {\n",
    "                    \"type\": \"date\"\n",
    "                },\n",
    "                \"field4\": {\n",
    "                    \"type\": \"boolean\"\n",
    "                },\n",
    "                \"field5\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"field6\": {\n",
    "                    \"type\": \"text\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create the index\n",
    "    es.indices.create(index=index_name, body=settings)\n",
    "\n",
    "\n",
    "def create_docs(start, end, index_name, last_doc):\n",
    "        # Generate the dummy data\n",
    "        dummy_data = []\n",
    "        for j in range(start, end):  # This will generate 1 million documents\n",
    "            doc = {\n",
    "                \"field1\": f\"dummy text {j}\",\n",
    "                \"field2\": j,\n",
    "                \"field3\": \"2022-01-01\",\n",
    "                \"field4\": True,\n",
    "                \"field5\": \"The problem is, you are setting the dataset to chart and then you are setting its X Range instead of setting X Range before setting the dataset to chart. You can add either following code in else part at the end: horizontalBarChart.invalidate(); horizontalBarChart.refreshDrawableState(); Or following code in your initCombinedChart: horizontalBarChart.setVisibleXRangeMaximum(10); horizontalBarChart.setVisibleXRangeMinimum(7); Edit 1: Try with setVisibleYRangeMaximum instead of setVisibleXRangeMaximum. \",\n",
    "                \"field6\": \"I am using IBM MobileFirst 7.0. I am getting following error message when I invoke a procedure. [6/6/16 15:13:18:516 IST] 000000e6 DataAccessSer E logError FWLSE0099E: An error occurred while invoking procedure [project PNotifications]AllInOneAdapter/HttpRequestFWLSE0100E: parameters: [project PNotifications] Http request failed: java.net.SocketTimeoutException: Read timed out FWLSE0101E: Caused by: [project PNotifications]java.net.SocketTimeoutException: Read timed outjava.lang.RuntimeException: Http request failed: java.net.SocketTimeoutException: Read timed out Below is log of adapter : [6/6/16 15:13:18:519 IST] 000000e6 JavaScriptInt I com.worklight.integration.js.JavaScriptIntegrationLibraryImplementation info { \\\"errors\\\": [ \\\"Runtime: Http request failed: java.net.SocketTimeoutException: Read timed out\\\" ], \\\"info\\\": [ ], \\\"isSuccessful\\\": false, \\\"warnings\\\": [ ] } [project PNotifications] I am facing this issue intermittently on the QA server which is at client-side. The app works perfectly at times, before this issue occurs randomly. I never faced this issue in the development environment. I also tried increasing the connectionTimeoutInMilliseconds value to 90000 but the issue still persists. As this issue occurs randomly we haven't been able to find the cause for it. Any help would be much appreciated. \"\n",
    "            }\n",
    "            dummy_data.append(doc)\n",
    "\n",
    "        chunk = dummy_data\n",
    "        actions = []\n",
    "        for doc in chunk:\n",
    "            action = {\n",
    "                \"index\": {\n",
    "                    \"_index\": index_name\n",
    "                }\n",
    "            }\n",
    "            actions.append(action)\n",
    "            actions.append(doc)\n",
    "        print(f'{end} docs added / {last_doc}')\n",
    "        es.bulk(body=actions)\n",
    "\n",
    "\n",
    "awsauth = aws_client()\n",
    "es = es_client(awsauth)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Program starts here, create_doc fails if i define a main method\n",
    "    args = parse_args()\n",
    "    # For 100GB\n",
    "    #num_docs = 674000000\n",
    "    num_docs = args['total']\n",
    "    # Use the bulk API to index the dummy data in chunks\n",
    "    chunk_size = args['chunk_size']\n",
    "    # Number of machines this workload will be divided up by\n",
    "    num_machines = args['num_machines']\n",
    "    machine_id = args['machine_id']\n",
    "    index_name = args['index_name']\n",
    "\n",
    "    #awsauth = aws_client()\n",
    "    #es = es_client(awsauth)\n",
    "    # Create the index\n",
    "    if args['create_index']:\n",
    "        create_index(index_name, es)\n",
    "\n",
    "    first_doc = (num_docs // num_machines) * (machine_id - 1)\n",
    "    last_doc = (num_docs // num_machines) * machine_id\n",
    "\n",
    "    # Start timer when adding docs to index\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a ProcessPoolExecutor with the desired number of processes\n",
    "    with cf.ProcessPoolExecutor(max_workers=args['max_workers']) as executor:\n",
    "        futures = []\n",
    "        for i in range(first_doc, last_doc, chunk_size):\n",
    "            # Don't go over the limit\n",
    "            ending_doc = min(i + chunk_size, last_doc)\n",
    "            futures.append(executor.submit(create_docs, i, ending_doc, index_name, last_doc))\n",
    "\n",
    "        cf.wait(futures)\n",
    "\n",
    "    # Print end time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Elapsed time: {elapsed_time:.2f} seconds')\n",
    "\n",
    "    # Refresh the index to make the indexed data searchable\n",
    "    es.indices.refresh(index=index_name)\n",
    "\n",
    "    # Check the size of the index\n",
    "    index_stats = es.indices.stats(index=index_name)\n",
    "    index_size_bytes = index_stats['indices'][index_name]['total']['store']['size_in_bytes']\n",
    "    index_size_gb = index_size_bytes / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "\n",
    "    # Print the size of the index\n",
    "    print(f\"Index size: {index_size_gb:.2f} GB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "This script can be deployed to multiple machines and make use of multiple cores per machine. \n",
    "\n",
    "Steps to start using the script are:\n",
    "\n",
    "- Download this python script to each machine you want this to run on\n",
    "- pip3 install all of these python packages\n",
    "```\n",
    "elasticsearch==7.10 # or whatever version you're using\n",
    "requests\n",
    "requests_aws4auth\n",
    "boto3\n",
    "```\n",
    "- `export DOMAIN=https://cluster_endpoint`\n",
    "- Run this script with required command line arguments\n",
    "\n",
    "## Command Line Args\n",
    "\n",
    "There are multiple command line arguments involved. All are required except for `create_index` which only needs to be used on one node.\n",
    "\n",
    "- total: total number of docs to create\n",
    "- chunk_size: Use Bulk API to index dummy data into specfied chunks\n",
    "- num_machines: Number of machines to distribute the load onto\n",
    "- machine_id: Id of machine (should be > 0 and less than or equal to num_machines\n",
    "- index_name: Name of index to add docs to\n",
    "- max_workers: Max number of workers or processes to spawn\n",
    "- create_index: Including this means this instance of the script will create the index (only the first machine should)\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "Let's say we want to run this tool across 4 machines. Let's send 2,300,000 docs in total, and we only want to send 10000 docs at a time for stability, and only use 10 cpu cores at any given time.\n",
    "\n",
    "This would be the layout across 4 different machines.\n",
    "\n",
    "```\n",
    "export DOMAIN=https://opensearch_cluster_endpoint.com/\n",
    "```\n",
    "\n",
    "Machine 1:\n",
    "```\n",
    "python3 create_index_multithreaded.py --total 2300000 --chunk_size 10000 --num_machines 4 --machine_id 1 --index_name test100gb-index --max_workers=10 --create_index\n",
    "```\n",
    "Machine 2:\n",
    "```\n",
    "python3 create_index_multithreaded.py --total 2300000 --chunk_size 10000 --num_machines 4 --machine_id 2 --index_name test100gb-index --max_workers=10\n",
    "```\n",
    "Machine 3:\n",
    "```\n",
    "python3 create_index_multithreaded.py --total 2300000 --chunk_size 10000 --num_machines 4 --machine_id 3 --index_name test100gb-index --max_workers=10\n",
    "```\n",
    "Machine 4:\n",
    "```\n",
    "python3 create_index_multithreaded.py --total 2300000 --chunk_size 10000 --num_machines 4 --machine_id 4 --index_name test100gb-index --max_workers=10\n",
    "```\n",
    "The four machines will:\n",
    "- Divide up the workload of 2,300,000 docs among the 4 machines equally using dummy data\n",
    "- Show progress of docs added to the index after each chunk is bulk added\n",
    "- Print out the total time the operation took\n",
    "- Print the size of the index created\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
